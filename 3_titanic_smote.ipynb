{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "One method to have better test score from classification model is to oversample the imbalanced target values. As the number of survived is different from the people who did not survive, we will oversample the data to have both categories have same numbers for both survived and not survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# drop irrelevant columns\n",
    "df.drop(columns = [\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\"], inplace=True)\n",
    "\n",
    "# handling missing values\n",
    "df.Age.fillna(df.Age.mean(), inplace=True)\n",
    "df.Embarked.fillna('N/A', inplace=True)\n",
    "\n",
    "# separating target and features\n",
    "X = df.drop(columns = [\"Survived\"])\n",
    "y = df.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Counts\n",
      "0    421\n",
      "1    247\n",
      "Name: Survived, dtype: int64\n",
      "\n",
      "Percentages\n",
      "0    0.63024\n",
      "1    0.36976\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Resampled Counts\n",
      "1    421\n",
      "0    421\n",
      "Name: Survived, dtype: int64\n",
      "\n",
      "Percentages\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# split train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "X_train_labeled = X_train.copy()\n",
    "X_test_labeled = X_test.copy()\n",
    "\n",
    "# The gender and embarked columns are labeled as numeric values\n",
    "col = [\"Sex\", \"Embarked\"]\n",
    "for c in col:\n",
    "    X_train_labeled[c] = le.fit_transform(X_train[c].astype('str'))\n",
    "    X_test_labeled[c] = le.transform(X_test[c].astype('str'))\n",
    "\n",
    "# oversampling    \n",
    "from imblearn.over_sampling import SMOTE \n",
    "smote = SMOTE(random_state=1)\n",
    "X_train_resampled, y_train_resampled = smote.fit_sample(X_train_labeled, y_train) \n",
    "\n",
    "# comparison before and after oversampling\n",
    "print(\"Raw Counts\")\n",
    "print(y_train.value_counts())\n",
    "print()\n",
    "print(\"Percentages\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Resampled Counts\")\n",
    "print(y_train_resampled.value_counts())\n",
    "print()\n",
    "print(\"Percentages\")\n",
    "print(y_train_resampled.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of survived passenger is now equal to the number of passengers who did not survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# importing saved function to show scores\n",
    "import data_preparation as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(random_state=1)\n",
      "\n",
      "CV score:     80.73%\n",
      "X-test score: 79.82%\n",
      "RMSE:         0.4492\n",
      "\n",
      "Train score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81       421\n",
      "           1       0.81      0.81      0.81       421\n",
      "\n",
      "    accuracy                           0.81       842\n",
      "   macro avg       0.81      0.81      0.81       842\n",
      "weighted avg       0.81      0.81      0.81       842\n",
      "\n",
      "\n",
      "\n",
      "X-test score\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       128\n",
      "           1       0.77      0.75      0.76        95\n",
      "\n",
      "    accuracy                           0.80       223\n",
      "   macro avg       0.79      0.79      0.79       223\n",
      "weighted avg       0.80      0.80      0.80       223\n",
      "\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(random_state=1)\n",
      "\n",
      "CV score:     82.94%\n",
      "X-test score: 72.65%\n",
      "RMSE:         0.523\n",
      "\n",
      "Train score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       421\n",
      "           1       1.00      0.99      0.99       421\n",
      "\n",
      "    accuracy                           0.99       842\n",
      "   macro avg       0.99      0.99      0.99       842\n",
      "weighted avg       0.99      0.99      0.99       842\n",
      "\n",
      "\n",
      "\n",
      "X-test score\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       128\n",
      "           1       0.69      0.64      0.67        95\n",
      "\n",
      "    accuracy                           0.73       223\n",
      "   macro avg       0.72      0.72      0.72       223\n",
      "weighted avg       0.72      0.73      0.72       223\n",
      "\n",
      "\n",
      "\n",
      "RandomForestClassifier(random_state=1)\n",
      "\n",
      "CV score:     83.89%\n",
      "X-test score: 77.13%\n",
      "RMSE:         0.4782\n",
      "\n",
      "Train score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       421\n",
      "           1       0.99      0.99      0.99       421\n",
      "\n",
      "    accuracy                           0.99       842\n",
      "   macro avg       0.99      0.99      0.99       842\n",
      "weighted avg       0.99      0.99      0.99       842\n",
      "\n",
      "\n",
      "\n",
      "X-test score\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       128\n",
      "           1       0.77      0.66      0.71        95\n",
      "\n",
      "    accuracy                           0.77       223\n",
      "   macro avg       0.77      0.76      0.76       223\n",
      "weighted avg       0.77      0.77      0.77       223\n",
      "\n",
      "\n",
      "\n",
      "AdaBoostClassifier(random_state=1)\n",
      "\n",
      "CV score:     83.73%\n",
      "X-test score: 75.78%\n",
      "RMSE:         0.4921\n",
      "\n",
      "Train score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86       421\n",
      "           1       0.87      0.85      0.86       421\n",
      "\n",
      "    accuracy                           0.86       842\n",
      "   macro avg       0.86      0.86      0.86       842\n",
      "weighted avg       0.86      0.86      0.86       842\n",
      "\n",
      "\n",
      "\n",
      "X-test score\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.80       128\n",
      "           1       0.74      0.67      0.70        95\n",
      "\n",
      "    accuracy                           0.76       223\n",
      "   macro avg       0.75      0.75      0.75       223\n",
      "weighted avg       0.76      0.76      0.76       223\n",
      "\n",
      "\n",
      "\n",
      "GradientBoostingClassifier(random_state=1)\n",
      "\n",
      "CV score:     84.68%\n",
      "X-test score: 81.61%\n",
      "RMSE:         0.4288\n",
      "\n",
      "Train score\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       421\n",
      "           1       0.94      0.88      0.91       421\n",
      "\n",
      "    accuracy                           0.91       842\n",
      "   macro avg       0.92      0.91      0.91       842\n",
      "weighted avg       0.92      0.91      0.91       842\n",
      "\n",
      "\n",
      "\n",
      "X-test score\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85       128\n",
      "           1       0.86      0.68      0.76        95\n",
      "\n",
      "    accuracy                           0.82       223\n",
      "   macro avg       0.83      0.80      0.81       223\n",
      "weighted avg       0.82      0.82      0.81       223\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "dp.scores(X_train_resampled, y_train_resampled, X_test_labeled, y_test, lr)\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=1)\n",
    "dp.scores(X_train_resampled, y_train_resampled, X_test_labeled, y_test, dtc)\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=1)\n",
    "dp.scores(X_train_resampled, y_train_resampled, X_test_labeled, y_test, rfc)\n",
    "\n",
    "abc = AdaBoostClassifier(random_state=1)\n",
    "dp.scores(X_train_resampled, y_train_resampled, X_test_labeled, y_test, abc)\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=1)\n",
    "dp.scores(X_train_resampled, y_train_resampled, X_test_labeled, y_test, gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test scores are generally increased, and gradient boosting still has the highest test scores. We will do tuning using gradient boosting model to see if we can improve test score in the next part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
